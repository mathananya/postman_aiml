{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0024e836",
   "metadata": {},
   "source": [
    "SUBMISSION -- ANANYA MUKHERJEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac487d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             customer_id                customer_unique_id  \\\n",
      "count                              99441                             99441   \n",
      "unique                             99441                             96096   \n",
      "top     06b8999e2fba1a1fbc88172c00ba8bc7  8d50f5eadf50201ccdcedfb9e2ac8455   \n",
      "freq                                   1                                17   \n",
      "\n",
      "       customer_city customer_state  \n",
      "count          99441          99441  \n",
      "unique          4119             27  \n",
      "top        sao paulo             SP  \n",
      "freq           15540          41746  \n",
      "                                order_id                       customer_id  \\\n",
      "count                              99441                             99441   \n",
      "unique                             99441                             99441   \n",
      "top     e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "freq                                   1                                 1   \n",
      "\n",
      "       order_status order_purchase_timestamp  \n",
      "count         99441                    99441  \n",
      "unique            8                    98875  \n",
      "top       delivered      2018-08-02 12:05:26  \n",
      "freq          96478                        3  \n",
      "       payment_value\n",
      "count  103886.000000\n",
      "mean      154.100380\n",
      "std       217.494064\n",
      "min         0.000000\n",
      "25%        56.790000\n",
      "50%       100.000000\n",
      "75%       171.837500\n",
      "max     13664.080000\n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "\n",
      "  order_status order_purchase_timestamp                customer_unique_id  \\\n",
      "0    delivered      2017-10-02 10:56:33  7c396fd4830fd04220f754e42b4e5bff   \n",
      "1    delivered      2017-10-02 10:56:33  7c396fd4830fd04220f754e42b4e5bff   \n",
      "2    delivered      2017-10-02 10:56:33  7c396fd4830fd04220f754e42b4e5bff   \n",
      "3    delivered      2018-07-24 20:41:37  af07308b275d755c9edb36a90c618231   \n",
      "4    delivered      2018-08-08 08:38:49  3a653a41f6f9fc3d2a113cf8398680e8   \n",
      "\n",
      "  customer_city customer_state  payment_value  \n",
      "0     sao paulo             SP          18.12  \n",
      "1     sao paulo             SP           2.00  \n",
      "2     sao paulo             SP          18.59  \n",
      "3     barreiras             BA         141.46  \n",
      "4    vianopolis             GO         179.12  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_cust = pd.read_csv('.\\\\archive\\\\olist_customers_dataset.csv')\n",
    "#print(df_cust.head())\n",
    "df_order = pd.read_csv('.\\\\archive\\\\olist_orders_dataset.csv')\n",
    "#print(df_order.head())\n",
    "df_pay = pd.read_csv('.\\\\archive\\\\olist_order_payments_dataset.csv')\n",
    "#print(df_pay.head())\n",
    "\n",
    "#keeping only select required columns\n",
    "df_cust = df_cust[['customer_id','customer_unique_id','customer_city','customer_state']]\n",
    "df_order = df_order[['order_id','customer_id','order_status','order_purchase_timestamp']]\n",
    "df_pay = df_pay[['order_id','payment_value']]\n",
    "print(df_cust.describe())\n",
    "print(df_order.describe())\n",
    "print(df_pay.describe())\n",
    "\n",
    "# doing join of df_order and df_customers on customer_id\n",
    "df_merged = pd.merge(df_order, df_cust, on='customer_id', how='inner')\n",
    "# doing join of df_merged and df_pay on order_id\n",
    "df_merged = pd.merge(df_merged, df_pay, on='order_id', how='inner')\n",
    "print(df_merged.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da014c",
   "metadata": {},
   "source": [
    "<h3>Notes on Data</h3>\n",
    "\n",
    "There are 99441 instances of customer ids, which correspond to 96096 unique customer ids. These customers belong to 4119 cities across 27 states. \n",
    "\n",
    "There are 99441 unique order ids, with the status belonging to 8 categories\n",
    "\n",
    "There are 1038886 payments with Mean payment value is 154.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7d37c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-29 15:00:37\n"
     ]
    }
   ],
   "source": [
    "#keeping only orders with status = 'delivered'\n",
    "df_merged = df_merged[df_merged['order_status']=='delivered']\n",
    "\n",
    "#convert order_purchase_timestamp to datetime\n",
    "df_merged['order_purchase_timestamp'] = pd.to_datetime(df_merged['order_purchase_timestamp'])\n",
    "\n",
    "#create reference date as max order_purchase_timestamp\n",
    "reference_date = df_merged['order_purchase_timestamp'].max()\n",
    "print(reference_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88d784",
   "metadata": {},
   "source": [
    "<h3>Handling Edge cases: </h3>\n",
    "Multiple payments for 1 order id - since the feature of Frequency is based by counting unique order ids, the payments will be aggregated and therefore the aggregation of payments to an order_id does not need to be handled separately\n",
    "Orders with non-delivered status have been excluded above - only rows of df_merged where order_status=='delivered' is selected\n",
    "Customers with no delivered orders will also have been eliminated from the rows of df_merged with the above selection\n",
    "\n",
    "<b>On unique customer ids :</b> There are 96096 unique_customer_id, for 99441 customer ids. Since we are analyzing the behaviour of a customer (and not an id), we should use the unique identifier to aggregate all his/her transactions. Hence we use this for creating features in subsequent steps.\n",
    "\n",
    "<h4>Feature engineering:</h4>\n",
    "\n",
    "Recency, Frequency and Monetary Value are created and stored in a new dataframe df_rfm\n",
    "Finally the target label 'churn' is created - which in our case is directly dependent on the value of one independent variable 'recency'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb57ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 customer_unique_id  recency  frequency  monetary_value  churn\n",
      "0  0000366f3b9a7992bf8c76cfdf3221e2      111          1          141.90      0\n",
      "1  0000b849f77a49e4a4ce2b2a4ca5be3f      114          1           27.19      0\n",
      "2  0000f46a3911fa3c0805444483337064      536          1           86.22      1\n",
      "3  0000f6ccb0745a6a4b88665a16c9f078      320          1           43.62      1\n",
      "4  0004aac84e0df4da2b147fca70cf8255      287          1          196.89      1\n"
     ]
    }
   ],
   "source": [
    "#develop features of recency, frequency, monetary value for each customer_unique_id\n",
    "#group by customer_unique_id\n",
    "df_rfm = df_merged.groupby('customer_unique_id').agg({\n",
    "    'order_purchase_timestamp': lambda x: (reference_date - x.max()).days,\n",
    "    'order_id': 'nunique',\n",
    "    'payment_value': 'sum'\n",
    "}).reset_index()\n",
    "df_rfm.columns = ['customer_unique_id', 'recency', 'frequency', 'monetary_value']\n",
    "\n",
    "#add a target label churn if recency > 180 days\n",
    "df_rfm['churn'] = df_rfm['recency'].apply(lambda x: 1 if x > 180 else 0)\n",
    "\n",
    "print(df_rfm.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c23f1",
   "metadata": {},
   "source": [
    ". \n",
    "<h3>Logistic Regression</h3>\n",
    "\n",
    "Classifier model has been trained with 70% of the data - 30%  kept aside for test. (random seed = 42 used to generate the train vs test data sets). \n",
    "All 3 features have been used to predict churn in this model. It has given a high accuracy of nearly 100%.\n",
    "\n",
    "However, should we have used 'recency' as a independent variable? Such a model will clearly show high accuracy as recency fully predicts churn. \n",
    "A more realistic model would be to try build a classifier of 'churn', based only on 'frequency' and 'monetary value'. \n",
    "This has been implemented below too, and (obviously) shows a lower accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4598b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9998928877463582\n",
      "Precision: 0.9999396900066341\n",
      "Recall: 0.9998793872874201\n",
      "F1-score: 0.9999095377378403\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11425     1]\n",
      " [    2 16580]]\n"
     ]
    }
   ],
   "source": [
    "# develop a logistic regression model to predict churn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#split the data into train and test\n",
    "X = df_rfm[['recency','frequency', 'monetary_value']]  #features including recency\n",
    "y = df_rfm['churn']  #target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "#predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# report accuracy, precision, recall, f1-score\n",
    "accuracy = (y_test == y_pred).mean()\n",
    "print(f'Accuracy: {accuracy}')\n",
    "precision = (y_test[y_pred == 1] == 1).mean()\n",
    "print(f'Precision: {precision}')\n",
    "recall = (y_pred[y_test == 1] == 1).mean()\n",
    "print(f'Recall: {recall}')\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1-score: {f1_score}')\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03d03d",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression - part 2</h3>\n",
    "\n",
    "Only 2 features have been used to predict churn in this model. I have deliberately left out 'recency' as that is obviously the complete predictor of churn in my training data. This model has given a much lower accuracy of around 60%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38fd2d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5922593544701514\n",
      "Precision: 0.5923038409269723\n",
      "Recall: 0.998793872874201\n",
      "F1-score: 0.7436242816091954\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   26 11400]\n",
      " [   20 16562]]\n"
     ]
    }
   ],
   "source": [
    "#split the data into train and test\n",
    "X = df_rfm[['frequency', 'monetary_value']]  #features excluding recency\n",
    "y = df_rfm['churn']  #target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "#predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# report accuracy, precision, recall, f1-score\n",
    "accuracy = (y_test == y_pred).mean()\n",
    "print(f'Accuracy: {accuracy}')\n",
    "precision = (y_test[y_pred == 1] == 1).mean()\n",
    "print(f'Precision: {precision}')\n",
    "recall = (y_pred[y_test == 1] == 1).mean()\n",
    "print(f'Recall: {recall}')\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1-score: {f1_score}')\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e0826",
   "metadata": {},
   "source": [
    "<h3>Random Forest Classification</h3>\n",
    "\n",
    "Classifier model has been trained with 70% of the data - 30%  kept aside for test. (random seed = 42 used to generate the train vs test data sets). \n",
    "All 3 features have been used to predict churn in this model. It has given a high accuracy of 100% (better than the logistic regression scores).\n",
    "\n",
    "However, a more realistic model to build a classifier of 'churn', based only on 'frequency' and 'monetary value' has been implemented below too, and (obviously) shows a lower accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e9ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11426     0]\n",
      " [    0 16582]]\n"
     ]
    }
   ],
   "source": [
    "#develop a random forest classifier for churn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#split the data into train and test\n",
    "X = df_rfm[['recency','frequency', 'monetary_value']]  #features including recency\n",
    "y = df_rfm['churn']  #target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#train the random  forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "#predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "# report accuracy, precision, recall, f1-score\n",
    "accuracy = (y_test == y_pred).mean()\n",
    "print(f'Accuracy: {accuracy}')\n",
    "precision = (y_test[y_pred == 1] == 1).mean()\n",
    "print(f'Precision: {precision}')\n",
    "recall = (y_pred[y_test == 1] == 1).mean()\n",
    "print(f'Recall: {recall}')\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1-score: {f1_score}')\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259c9ea4",
   "metadata": {},
   "source": [
    "<h3>Random Forest - part 2</h3>\n",
    "\n",
    "Only 2 features have been used to predict churn in this model. I have deliberately left out 'recency' as that is obviously the complete predictor of churn in my training data. This model has given a much lower accuracy of around 74%. This is again higher than the logistic regression model made with 2 independent features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e8d5cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7370751213938874\n",
      "Precision: 0.7731421121251629\n",
      "Recall: 0.7867567241587263\n",
      "F1-score: 0.7798900047824007\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 7598  3828]\n",
      " [ 3536 13046]]\n"
     ]
    }
   ],
   "source": [
    "#split the data into train and test\n",
    "X = df_rfm[['frequency', 'monetary_value']]  #features excluding recency\n",
    "y = df_rfm['churn']  #target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#train the random  forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "#predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "# report accuracy, precision, recall, f1-score\n",
    "accuracy = (y_test == y_pred).mean()\n",
    "print(f'Accuracy: {accuracy}')\n",
    "precision = (y_test[y_pred == 1] == 1).mean()\n",
    "print(f'Precision: {precision}')\n",
    "recall = (y_pred[y_test == 1] == 1).mean()\n",
    "print(f'Recall: {recall}')\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1-score: {f1_score}')\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451c8fc",
   "metadata": {},
   "source": [
    "<h3>Reproducibility notes </h3>\n",
    "\n",
    "<b>Random seed</b> used has been 42 in all cases - for splitting train/test samples and for the random forest state\n",
    "\n",
    "<b>Python libraries</b> used are pandas version 2.3.2 , scikit-learn / sklearn version 1.7.2\n",
    "\n",
    "<b>Python</b> version 3.13.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
